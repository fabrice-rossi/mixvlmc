---
title: "Sampling from (CO)VLMC"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Sampling from (CO)VLMC}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache.path = ".sampling_cache/"
)
```

```{r setup, message=FALSE}
library(mixvlmc)
library(ggplot2)
library(data.table) ## for frollapply
```

Once a (CO)VLMC has been estimated from a sequence, it can be used to produce new sequences. Any statistical estimator defined on the original sequence can be applied to the simulated sequences leading a form of [semi-parametric bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)). This provides
an interesting alternative to other boostrapping solutions for correlated data,
mainly the block bootstrap. 

## Theoretical results

BÃ¼hlmann and Wyner show in [their paper](https://dx.doi.org/10.1214/aos/1018031204)
that this approach is consistent for statistics that depend smoothly from arbitrary
means of fixed mappings of contexts to numerical values. In simple terms, one can
first choose a mapping of arbitrary tuples of values in the state space to numerical 
vectors and then build a estimator by averaging those vectors over the observed sequence.
The resulting vector mean can then be transformed smoothly to lead to an estimator.
The VLMC bootstrap is then consistent for this estimator under relatively mild
hypothesis on the underlying VLMC. 

A typical example of such estimator is the probability of observing a fixed 
pattern in the sequence. More generally, the class of estimators includes smooth
transformations of the empirical distribution of patterns up to a fixed length. 

In practice, it is recommended to sample a longer sequence than the one needed and
to keep only the last $n$ values, in order to minimise the influence of the starting
values. 

## Sampling from a VLMC
### Introduction

Let us consider a simple example with an independent sample:
```{r}
dts <- sample(0:1, 500, replace = TRUE)
```
The optimal VLMC according to the BIC has non memory:
```{r}
best_vlmc <- tune_vlmc(dts)
draw(as_vlmc(best_vlmc))
```

Simulating a sequence using the model is done via the standard `stats::simulate()` 
function as follows:

```{r}
dts_sample <- simulate(as_vlmc(best_vlmc), 600)[-(1:100)]
```

Even if this is useless here because of the independence, we drop the first 100
samples. In general `simulate.vlmc()` is used as `stats::simulate()` and supports 
the standard parameters:

- `nsim` for the number of simulated values, here the length of the new time series
- `seed` to specify the random seed used during the simulation (the initial state of
   the random number generator is restored to its previous after simulating the values)
   
In addition, one can specify the initial values of the sequence via the `init` parameter, for
instance:
```{r}
dts_sample_2 <- simulate(as_vlmc(best_vlmc), 10, init=c(0L, 0L))
```
is guaranteed to start with $0, 0$:
```{r}
dts_sample_2
```
This provides an alternative to dropping the initial values of the simulated time
series by setting those values to the one observed in the data set. Notice that this
practice has no theoretical justification. 

### CAC time series
Let us consider the French CAC index provided in `EuStockMarkets`:
```{r}
CAC_raw <- as.data.frame(EuStockMarkets)$CAC
```
We turn it into a discrete time series with three values:

-  Stay if the value of the index on day t+1 is between 99.5% and 100.5% of the value
on day t
-  Up if the value increased by at least 0.5%
-  Down if the value decreased by at least 0.5%

```{r}
CAC_rel_evol <- diff(CAC_raw) / CAC_raw[-length(CAC_raw)]
CAC_dts <- factor(
  ifelse(CAC_rel_evol >= 0.005, "Up",
    ifelse(CAC_rel_evol <= -0.005, "Down", "Stay")
  ),
  levels = c("Down", "Stay", "Up"),
  ordered = TRUE
)
```

Then we adjust a VLMC to the time series using the AIC criterion:
```{r cache=TRUE}
CAC_vlmc <- tune_vlmc(CAC_dts, criterion = "AIC")
CAC_model <- as_vlmc(CAC_vlmc)
```

We use here the AIC to favour predictive performances and as a consequence the
obtained model is quite complex with `r context_number(CAC_model)` contexts. 

The original discrete time series does not exhibit long subsequences of constant 
values as shown in the following graphical representation.
```{r}
CAC_rle <- rle(as.integer(CAC_dts))
CAC_rle_df <- data.frame(value = CAC_rle$values, length = CAC_rle$lengths)
ggplot(CAC_rle_df, aes(x = length)) +
  geom_bar() +
  labs(
    title = "Distribution of the lengths of constant subsequences",
    x = "Length",
    y = "Count"
  )
```

In this context, a natural statistics is the probability of observing a constant
subsequence of a length between 5 and 10 for instance. Notice that the patterns
used in the statistics must be of fixed lengths to be covered by the consistency
theorems mentioned above. We implement the statistics as follows:

```{r}
long_sequence <- function(dts) {
  dts_int <- as.integer(dts)
  ## RLE cannot be used directly as we need to account for overlapping 
  ## patterns
  dts_freq <- frollapply(dts_int, 10, \(x) max(rle(x)$length)>=5)
  mean(dts_freq, na.rm=TRUE)
}
```

We generate 100 bootstrap samples, using a burning time proportional to the 
depth of the model, in order to allow for a proper mixing to take place. We use
the `seed` parameter of `stats::simulate()` to ensure reproducibility. 

```{r cache=TRUE}
bootstrap_samples <- vector(100, mode="list")
burning_time <- 50 * depth(CAC_model)
for(b in seq_along(bootstrap_samples)) {
  bootstrap_samples[[b]] <- simulate(CAC_model, 
                                     burning_time + length(CAC_dts), 
                                     seed = b)[-(1:burning_time)]
}
```

Then we compute the statistics on the bootstrap samples. 
```{r cache=TRUE}
bootstrap_ls <- sapply(bootstrap_samples, long_sequence)
```

The bootstrap distribution of this statistics is illustrated on the following
figure in which the red vertical line represents the value of the statistics on
the original sequence. 
```{r}
ggplot(mapping=aes(x=bootstrap_ls)) +
  geom_density() +
  geom_rug() +
  labs(title="Bootstrap distribution of the probability of long constant subsequences",
       x="Probability") +
  geom_vline(xintercept = long_sequence(CAC_dts), color = "red")
```

